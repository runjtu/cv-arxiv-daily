import os
import re
import json
import arxiv
import yaml
import logging
import argparse
import datetime
import requests

logging.basicConfig(format='[%(asctime)s %(levelname)s] %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)

base_url = "https://arxiv.paperswithcode.com/api/v0/papers/"
github_url = "https://api.github.com/search/repositories"
arxiv_url = "http://arxiv.org/"

def load_config(config_file:str) -> dict:
    '''
    config_file: input config file path
    return: a dict of configuration
    '''
    # make filters pretty
    def pretty_filters(**config) -> dict:
        keywords = dict()
        EXCAPE = '\"'
        QUOTA = '' # NO-USE
        OR = 'OR' # TODO
        def parse_filters(filters:list):
            ret = ''
            for idx in range(0,len(filters)):
                filter = filters[idx]
                if len(filter.split()) > 1:
                    ret += (EXCAPE + filter + EXCAPE)  
                else:
                    ret += (QUOTA + filter + QUOTA)   
                if idx != len(filters) - 1:
                    ret += OR
            return ret
        for k,v in config['keywords'].items():
            keywords[k] = parse_filters(v['filters'])
        return keywords
    with open(config_file,'r') as f:
        config = yaml.load(f,Loader=yaml.FullLoader) 
        config['kv'] = pretty_filters(**config)
        logging.info(f'config = {config}')
    return config 

def get_authors(authors, first_author = False):
    output = str()
    if first_author == False:
        output = ", ".join(str(author) for author in authors)
    else:
        output = authors[0]
    return output

def sort_papers(papers):
    output = dict()

    # sort by time
    time_based_list = []
    for k, v in papers.items():
        # import pdb;pdb.set_trace()
        year_month_day = v.split("**")[1]
        time_based_list.append((year_month_day, k))
    
    time_based_list.sort(reverse=True)
    
    for _, key in time_based_list:
        output[key] = papers[key]
    return output


def get_code_link(qword:str) -> str:
    """
    This short function was auto-generated by ChatGPT. 
    I only renamed some params and added some comments.
    @param qword: query string, eg. arxiv ids and paper titles
    @return paper_code in github: string, if not found, return None
    """
    # query = f"arxiv:{arxiv_id}"
    query = f"{qword}"
    params = {
        "q": query,
        "sort": "stars",
        "order": "desc"
    }
    r = requests.get(github_url, params=params)
    results = r.json()
    code_link = None
    if results["total_count"] > 0:
        code_link = results["items"][0]["html_url"]
    return code_link

def update_paper_links(filename):
    '''
    weekly update paper links in json file 
    '''
    def parse_arxiv_string(s):
        parts = s.split("|")
        date = parts[1].strip()
        title = parts[2].strip()
        authors = parts[3].strip()
        arxiv_id = parts[4].strip()
        code = parts[5].strip()
        arxiv_id = re.sub(r'v\d+', '', arxiv_id)
        return date,title,authors,arxiv_id,code

    with open(filename,"r") as f:
        content = f.read()
        if not content:
            m = {}
        else:
            m = json.loads(content)
            
        json_data = m.copy() 

        for keywords,v in json_data.items():
            logging.info(f'keywords = {keywords}')
            for paper_id,contents in v.items():
                contents = str(contents)

                update_time, paper_title, paper_first_author, paper_url, code_url = parse_arxiv_string(contents)

                contents = "|{}|{}|{}|{}|{}|\n".format(update_time,paper_title,paper_first_author,paper_url,code_url)
                json_data[keywords][paper_id] = str(contents)
                logging.info(f'paper_id = {paper_id}, contents = {contents}')
                
                valid_link = False if '|null|' in contents else True
                if valid_link:
                    continue
                try:
                    code_url = base_url + paper_id #TODO
                    r = requests.get(code_url).json()
                    repo_url = None
                    if "official" in r and r["official"]:
                        repo_url = r["official"]["url"]
                        if repo_url is not None:
                            new_cont = contents.replace('|null|',f'|**[link]({repo_url})**|')
                            logging.info(f'ID = {paper_id}, contents = {new_cont}')
                            json_data[keywords][paper_id] = str(new_cont)

                except Exception as e:
                    logging.error(f"exception: {e} with id: {paper_id}")
        # dump to json file
        with open(filename,"w") as f:
            json.dump(json_data,f)

def update_json_file(filename,data_dict):
    '''
    daily update json file using data_dict
    '''
    with open(filename,"r") as f:
        content = f.read()
        if not content:
            m = {}
        else:
            m = json.loads(content)
            
    json_data = m.copy() 
    
    # update papers in each keywords         
    for data in data_dict:
        for keyword in data.keys():
            papers = data[keyword]

            if keyword in json_data.keys():
                json_data[keyword].update(papers)
            else:
                json_data[keyword] = papers

    with open(filename,"w") as f:
        json.dump(json_data,f)
    
def json_to_md(filename,
               md_filename,
               pdf_analysis_path,
               pdf_analysis_prompt,
               task = '',
               to_web = False, 
               use_title = True, 
               use_tc = True,
               show_badge = True,
               use_b2t = True):
    """
    @param filename: str
    @param md_filename: str
    @return None
    """
    def pretty_math(line:str, pdf_analysis_path:str) -> str:
        # Example:
        # |**2023-03-23**|**PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360$^{\circ}$**|Sizhe An et.al.|[2303.13071](http://arxiv.org/abs/2303.13071)|null|

        items = line.split('|')
        paper_time = items[1].replace("*", "")
        paper_title = items[2].replace("*", "")
        paper_auther = items[3].replace("*", "")
        paper_id = items[4].split(']', 1)[0][1:]
        paper_result_json_path = os.path.join(pdf_analysis_path, paper_id+".json")
        if not os.path.exists(paper_result_json_path):
            return ''
        paper_analysis = json.loads(open(paper_result_json_path).read())['response']

        new_line = f'<details><summary> <b>{paper_time} </b> {paper_title} ({paper_auther})  <a href="http://arxiv.org/pdf/{paper_id}.pdf">PDF</a> </summary>  <p> {paper_analysis} </p>  </details> \n\n'
        return new_line
  
    DateNow = datetime.date.today()
    DateNow = str(DateNow)
    DateNow = DateNow.replace('-','.')
    
    with open(filename,"r") as f:
        content = f.read()
        if not content:
            data = {}
        else:
            data = json.loads(content)

    # clean README.md if daily already exist else create it
    with open(md_filename,"w+") as f:
        pass

    # write data into README.md
    with open(md_filename,"a+") as f:

        if (use_title == True) and (to_web == True):
            f.write("---\n" + "layout: default\n" + "---\n\n")
        
        if show_badge == True:
            f.write(f"[![Contributors][contributors-shield]][contributors-url]\n")
            f.write(f"[![Forks][forks-shield]][forks-url]\n")
            f.write(f"[![Stargazers][stars-shield]][stars-url]\n")
            f.write(f"[![Issues][issues-shield]][issues-url]\n\n")    
        
        f.write("# vpr Paper AI Analysis \n")
        if use_title == True:
            #f.write(("<p align="center"><h1 align="center"><br><ins>vpr-arxiv-daily"
            #         "</ins><br>Automatically Update CV Papers Daily</h1></p>\n"))
            f.write("## Manually Updated on " + DateNow + "\n")
        else:
            f.write("> Manually Updated on " + DateNow + "\n")

        f.write("The content of this page is generated by [claude.ai](https://claude.ai/) (papers before 2024.02) or ChatGPT GPT4 (papers after 2024.02). \n\n")  
        f.write("**This page is currently **under construction** \n\n")
        f.write("**~~The cost of the API is high. I am exploring ways to manage it.~~  Thanks to OpenAI, the cost of the `GPT-4o-2024-05-13` API is now very low. \n\n")
        f.write("The content herein was generated from the following `prompt`: \n\n")
        pdf_analysis_prompt = pdf_analysis_prompt.replace("\n", "  \n")
        f.write(f"> {pdf_analysis_prompt}\n\n")
        f.write("The generated contents are not guaranteed to be 100\\% accurate. \n\n")
        f.write("[Back to the Paper Index](https://github.com/runjtu/vpr-arxiv-daily) \n\n")

        #Add: table of contents
        if use_tc == True:
            f.write("<details>\n")
            f.write("  <summary>Table of Contents</summary>\n")
            f.write("  <ol>\n")
            for keyword in data.keys():
                day_content = data[keyword]
                if not day_content:
                    continue
                kw = keyword.replace(' ','-')      
                f.write(f"    <li><a href=#{kw.lower()}>{keyword}</a></li>\n")
            f.write("  </ol>\n")
            f.write("</details>\n\n")
        
        for keyword in data.keys():
            day_content = data[keyword]
            if not day_content:
                continue
            
            # the head of each part
            f.write(f"## {keyword}\n\n")

            # if use_title == True :
                # if to_web == False:
                #     f.write("|Publish Date|Title|Authors|PDF|Code|\n" + "|---|---|---|---|---|\n")
                # else:
                #     f.write("| Publish Date | Title | Authors | PDF | Code |\n")
                #     f.write("|:---------|:-----------------------|:---------|:------|:------|\n")

            # sort papers by date
            day_content = sort_papers(day_content)
        
            for _,v in day_content.items():
                if v is not None:
                    if v.startswith("|**1") or v.startswith("|**200"): # delete too old papers with year under 2010
                        continue
                    content = pretty_math(v, pdf_analysis_path)
                    if content != '':
                        f.write(content) # make latex pretty

            f.write(f"\n")
            
            #Add: back to top
            if use_b2t:
                top_info = f"#Updated on {DateNow}"
                top_info = top_info.replace(' ','-').replace('.','')
                f.write(f"<p align=right>(<a href={top_info.lower()}>back to top</a>)</p>\n\n")
            
        if show_badge == True:
            # we don't like long string, break it!
            f.write((f"[contributors-shield]: https://img.shields.io/github/"
                     f"contributors/runjtu/vpr-arxiv-daily.svg?style=for-the-badge\n"))
            f.write((f"[contributors-url]: https://github.com/runjtu/"
                     f"vpr-arxiv-daily/graphs/contributors\n"))
            f.write((f"[forks-shield]: https://img.shields.io/github/forks/runjtu/"
                     f"vpr-arxiv-daily.svg?style=for-the-badge\n"))
            f.write((f"[forks-url]: https://github.com/runjtu/"
                     f"vpr-arxiv-daily/network/members\n"))
            f.write((f"[stars-shield]: https://img.shields.io/github/stars/runjtu/"
                     f"vpr-arxiv-daily.svg?style=for-the-badge\n"))
            f.write((f"[stars-url]: https://github.com/runjtu/"
                     f"vpr-arxiv-daily/stargazers\n"))
            f.write((f"[issues-shield]: https://img.shields.io/github/issues/runjtu/"
                     f"vpr-arxiv-daily.svg?style=for-the-badge\n"))
            f.write((f"[issues-url]: https://github.com/runjtu/"
                     f"vpr-arxiv-daily/issues\n\n"))
                
    logging.info(f"{task} finished")        

def demo(**config):
    # TODO: use config
    data_collector = []
    data_collector_web= []
    
    keywords = config['kv']

    show_badge = config['show_badge']

    json_file = '../' + config['json_readme_path']
    md_file   = '../' +config['claudeai_path']

    pdf_analysis_path = config['pdf_analysis_path']
    pdf_analysis_prompt = open(config['pdf_analysis_prompt_path']).read()

    json_to_md(json_file, md_file, pdf_analysis_path, pdf_analysis_prompt, task ='Update prompt', \
        show_badge = show_badge)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--config_path',type=str, default='../config.yaml',
                            help='configuration file path')
    parser.add_argument('--pdf_analysis_path',type=str, default='./results/claude_results/prompt1/',
                            help='pdf analysis content')
    parser.add_argument('--pdf_analysis_prompt_path',type=str, default='./results/claude_results/prompt1.txt',
                            help='pdf analysis content')
                      
    args = parser.parse_args()
    config = load_config(args.config_path)
    config['pdf_analysis_path'] = args.pdf_analysis_path
    config['pdf_analysis_prompt_path'] = args.pdf_analysis_prompt_path
    config = {**config}
    demo(**config)
